{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "数据模块可细分为4个部分：\n",
    "> 数据收集：标签和样本\n",
    "> - 数据划分：训练集、验证集和测试集\n",
    "> - 数据读取：对应于PyTorch的**DataLoader**。其中DataLoader包括**Sampler**和**DataSet**，Sampler的功能是生成索引，DataSet是根据生成的索引读样本以及标签。\n",
    "> - 数据预处理：对应于PyTorch的**transforms**\n",
    "\n",
    "---\n",
    "\n",
    "## DataLoader与DataSet\n",
    "`\n",
    "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "`\n",
    "\n",
    "功能：构建可迭代的数据装载器：\n",
    "> - dataset: Dataset 类，决定数据从哪里读取以及如何读取\n",
    "> - batchsize: 批大小\n",
    "> - num_works:num_works: 是否多进程读取数据\n",
    "> - sheuffle: 每个 epoch 是否乱序\n",
    "> - drop_last: 当样本数不能被 batchsize 整除时，是否舍弃最后一批数据\n",
    "\n",
    "## Epoch, Iteration, Batchsize\n",
    "> - Epoch: 所有训练样本都已经输入到模型中，称为一个Epoch\n",
    "> - Iteration: 一批样本输入到模型中，称为一个Iteration\n",
    "> - Batchsize: 批大小，决定一个iteration有多少样本，也决定了一个Epoch有多少个Iteration\n",
    "\n",
    "## torch.utils.data.Dataset\n",
    "功能：Dataset是抽象类，所有自定义的Dataset都需要继承该类，并且重写`__getitem()__`方法和`__len__()`方法。`__getitem()__`方法的作用是接收一个索引，返回索引对应的样本和标签，这是我们自己需要实现的逻辑。`__len__()`方法是返回所有样本的数量。\n",
    "数据读取包含3个方面:\n",
    "> - 读取哪些数据：每个 Iteration 读取一个Batchsize大小的数据，每个Iteration应该读取哪些数据。\n",
    "> - 从哪里读取数据：如何找到硬盘中的数据，应该在哪里设置文件路径参数\n",
    "> - 如何读取数据：不同的文件需要使用不同的读取方法和库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70ab94038f60fd04"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def get_img_info(data_dir):\n",
    "    '''\n",
    "    实现读取数据的Dataset，编写一个get_img_info()方法，读取每一个图片的路径和对应的标签，组成一个元组，再把所有的元组作为 list 存放到self.data_info变量中，这里需要注意的是标签需要映射到0开始的整数: rmb_label = {\"1\": 0, \"100\": 1}\n",
    "    :param data_dir: \n",
    "    :return: \n",
    "    '''\n",
    "    data_info = list()\n",
    "    for root, dirs, _ in os.walk(data_dir):\n",
    "        for sub_dir in dirs:\n",
    "            img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "            img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "            for i in range(len(img_names)):\n",
    "                img_name = img_names[i]\n",
    "                path_img = os.path.join(root, sub_dir, img_name)\n",
    "                label = rmb_label[sub_dir]\n",
    "                data_info.append((path_img, int(label)))\n",
    "    return data_info        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T00:46:03.040456400Z",
     "start_time": "2023-11-29T00:46:03.009211600Z"
    }
   },
   "id": "90606206a4ed3b00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "然后在Dataset的初始化函数中调用`get_img_info()`方法。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7841e176596f904c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __init__(self, data_dir, transform=None):\n",
    "    '''\n",
    "    rmb面额分类任务的Dataset\n",
    "    \n",
    "    :param self: \n",
    "    :param data_dir: str, 数据集所在路径\n",
    "    :param transform: torch.transform，数据预处理\n",
    "    :return: \n",
    "    '''\n",
    "    # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "    self.data_info = self.get_img_info(data_dir)\n",
    "    self.transform = transform"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26c968edc762f206"
  },
  {
   "cell_type": "markdown",
   "source": [
    "然后在`__getitem__()`方法中根据index 读取`self.data_info`中路径对应的数据，并在这里做transform操作，返回的是样本和标签。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f178402c6a250f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __getitem__(self, index):\n",
    "        # 通过index读取样本\n",
    "        path_img, label = self.data_info[index]\n",
    "        # 注意这里需要convert('RGB')\n",
    "        img = Image.open(path_img).convert('RGB') # 0~255\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) # 在这里做transform，转为tensor等等\n",
    "        # 返回是样本和标签\n",
    "        return img, label"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa457c64e876733"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在`__len__()`方法中返回`self.data_info`的长度，即为所有样本的数量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc0563043d7dfc1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 返回所有样本的数量\n",
    "def __len__(self):\n",
    "    return len(self.data_info)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T00:51:23.743913700Z",
     "start_time": "2023-11-29T00:51:23.728225700Z"
    }
   },
   "id": "2999a17828d652fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在`train_lenet.py`中，分5步构建模型。\n",
    "第1步设置数据。首先定义训练集、验证集、测试集的路径，定义训练集和测试集的`transforms`。然后构建训练集和验证集的`RMBDataset`对象，把对应的路径和`transforms`传进去。再构建`DataLoder`，设置`batch_size`，其中训练集设置`shuffle=True`，表示每个 Epoch都打乱样本。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237ddd05a8c7b67c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dr=train_dir, transform=train_transform)\n",
    "vaild_data = RMBDataset(data_dir=vaild_dir, transform=vaild_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "# 其中训练集设置shuffle=True，表示每个Epoch都打乱样本\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "vaild_loader = DataLoader(dataset=vaild_data, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e23e149ac210c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第2步构建模型，这里采用经典的Lenet图片分类网络。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f8d9b6a9263e27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e286e8f23d5cee23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第3步设置损失函数，这里使用交叉熵损失函数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c9ff7e647456d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff46ee93c4fde014"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第4步设置优化器。这里采用SGD优化器。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "880912cd3f167825"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 选择优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    " # 设置学习率下降策略\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15358692e5dab127"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第5步迭代训练模型，在每一个epoch里面，需要遍历train_loader取出数据，每次取得数据是一个batchsize大小。这里又分为4步。第1步进行前向传播，第2步进行反向传播求导，第3步使用optimizer更新权重，第4步统计训练情况。每一个epoch完成时都需要使用scheduler更新学习率，和计算验证集的准确率、loss。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3cb3965b36aa2ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    # 遍历 train_loader 取数据\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率\n",
    "    # 每个 epoch 计算验证集得准确率和loss\n",
    "    ...\n",
    "    ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "758ff68106de3234"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以看到每个iteration，我们是从train_loader中取出数据的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e85c5eff9960464e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __iter__(self):\n",
    "    if self.num_workers == 0:\n",
    "        return _SingleProcessDataLoaderIter(self)\n",
    "    else:\n",
    "        return _MultiProcessingDataLoaderIter(self)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3046b7368422845"
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我们没有设置多进程，会执行`_SingleProcessDataLoaderIter`的方法。我们以`_SingleProcessDataLoaderIter`为例。在`_SingleProcessDataLoaderIter`里只有一个方法`_next_data()`，如下："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "191a0be97a80e3ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _next_data(self):\n",
    "    index = self._next_index() # may raise StopIteration\n",
    "    data = self._dataset_fetcher.fetch(index) # may raise StopIteration\n",
    "    if self._pin_memory:\n",
    "        data = _utils.pin_memory.pin_memory(data)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5878c3aa3c652d03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在该方法中，`self._next_index()`是获取一个batchsize大小的index列表，代码如下："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcebfa6d55b096dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _next_index(self):\n",
    "    return next(self._sampler_iter)  # may raise StopIteration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7dbd298f51e323c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中调用的`sampler`类的`__iter__()`方法返回batch_size大小的随机index列表。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302fc181a5e92004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __iter__(self):\n",
    "    batch = []\n",
    "    for idx in self.sampler:\n",
    "        batch.append(idx)\n",
    "        if len(batch) == self.batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0 and not self.drop_last:\n",
    "        yield batch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e007d967c7a19b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "然后再返回看`dataloader`的`_next_data()`方法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd20857812124eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _next_data(self):\n",
    "    index = self._next_index()  # may raise StopIteration\n",
    "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
    "    if self._pin_memory:\n",
    "        data = _utils.pin_memory.pin_memory(data)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1f7971258886592"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在第二行中调用了`self._dataset_fetcher.fetch(index)`获取数据。这里会调用`_MapDatasetFetcher`中的`fetch()`函数："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c72bec213668be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch(self, possibly_batched_index):\n",
    "    if self.auto_collation:\n",
    "        data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "    else:\n",
    "        data = self.dataset[possibly_batched_index]\n",
    "    return self.collate_fn(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa9275f33e27ed52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里调用了`self.dataset[idx]`，这个函数会调用dataset.__getitem__()方法获取具体的数据，所以__getitem__()方法是我们必须实现的。我们拿到的data是一个list，每个元素是一个tunple，每个tunple包括样本和标签。所以最后要使用self.collate_fn(data)把data转换为两个list，第一个元素是样本的batch形式，形状为[16, 3, 32, 32](16是batch size，[3, 32, 32]是图片像素)；第二个元素是标签的batch形式，形状为[16]。\n",
    "所以在代码中，我们使用`inputs, labels = data`来接收数据。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7ef58f34d8be3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "首先在for循环中遍历`DataLoader`，然后根据是否采用多进程，决定使用单进程或者多进程的`DataLoaderIter`。在DataLoaderIter里调用`Sampler`生成Index的list，再调用`DatasetFetcher`根据index获取数据。在DatasetFetcher里会调用`Dataset`的`__getitem__()`方法获取真正的数据。这里获取的数据是一个list，其中每个元素是`(img, label)`的元组，再使用`collate_fn()`函数整理成一个list，里面包含**两个**元素，分别是img和label的`tenser`。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32242d4828263989"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
